{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13c23a03",
   "metadata": {},
   "source": [
    "# context \n",
    "\n",
    "This script performs the final prediction using the LSTM model selected during the model‐selection phase. It also implements our chosen strategies by fetching fresh data directly from the API, making the script fully self‐contained and runnable at any time to have recent strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9247c2d8",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebc1e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf122e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.tseries.offsets import BDay\n",
    "\n",
    "from pyspark.sql import SparkSession, Window, Row\n",
    "from pyspark.sql.types import StructType, StructField, DateType, DoubleType\n",
    "from pyspark.sql.functions import (\n",
    "    col, to_date, lag, avg, stddev, log, when, current_date,\n",
    "    date_sub, add_months, expr, row_number, dayofweek, lit,\n",
    "    pandas_udf, PandasUDFType\n",
    ")\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21372ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol =  \"AAPL\"\n",
    "api_key = \"UDEJQ3FLFIY9BVVT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9434df",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"AAPL_Prediction\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915d3bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alpha_vantage_data(symbol, key):\n",
    "    function = 'TIME_SERIES_DAILY'\n",
    "    output_size = 'full'\n",
    "    url = f'https://www.alphavantage.co/query?function={function}&symbol={symbol}&outputsize={output_size}&apikey={key}'\n",
    "\n",
    "    r = requests.get(url)\n",
    "    data = r.json()\n",
    "\n",
    "    if 'Time Series (Daily)' in data:\n",
    "        df = pd.DataFrame(data['Time Series (Daily)']).T\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "        df.sort_index(inplace=True)\n",
    "        df.columns = [col.split('. ')[1] for col in df.columns]\n",
    "        df = df.apply(pd.to_numeric)\n",
    "        df = df[df.index >= datetime.now() - timedelta(days=365*20)]\n",
    "\n",
    "        # Filter for data up to 18th of May 2025\n",
    "        cutoff_date = pd.Timestamp('2025-05-18')\n",
    "        df = df[df.index <= cutoff_date]\n",
    "\n",
    "        df.reset_index(inplace=True)\n",
    "        df.rename(columns={'index': 'Date'}, inplace=True)\n",
    "        return spark.createDataFrame(df)\n",
    "    else:\n",
    "        raise Exception(\"Alpha Vantage API Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb9f953",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = get_alpha_vantage_data(symbol, api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e29132a",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = {\n",
    "    '2005-02-28': 2, # two-for-one stock split on 28 February 2005\n",
    "    '2014-06-09': 7, #  seven-for-one stock split on 9th of June 2014\n",
    "    '2020-08-31': 4. #  four-for-one stock split on 31st of August 2020\n",
    "}\n",
    "\n",
    "for split_date_str, stock_split in splits.items():\n",
    "    split_date = datetime.strptime(split_date_str, \"%Y-%m-%d\")\n",
    "    \n",
    "    # Adjust prices before split date\n",
    "    spark_df = spark_df.withColumn(\n",
    "        'open',\n",
    "        when(col('Date') < lit(split_date), col('open') / stock_split).otherwise(col('open'))\n",
    "    ).withColumn(\n",
    "        'high',\n",
    "        when(col('Date') < lit(split_date), col('high') / stock_split).otherwise(col('high'))\n",
    "    ).withColumn(\n",
    "        'low',\n",
    "        when(col('Date') < lit(split_date), col('low') / stock_split).otherwise(col('low'))\n",
    "    ).withColumn(\n",
    "        'close',\n",
    "        when(col('Date') < lit(split_date), col('close') / stock_split).otherwise(col('close'))\n",
    "    ).withColumn(\n",
    "        'volume',\n",
    "        when(col('Date') < lit(split_date), col('volume') * stock_split).otherwise(col('volume'))\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ad197f",
   "metadata": {},
   "source": [
    "# Final Prediction\n",
    "\n",
    "We use the LSTM model chosen during model selection. Because Spark ML doesn’t support LSTMs, we:\n",
    "\n",
    "1. Convert the Spark DataFrame to pandas  \n",
    "2. Run the LSTM predictions in pandas  \n",
    "3. Convert the pandas results back into a Spark DataFrame  \n",
    "\n",
    "**What’s changed**\n",
    "\n",
    "- **Larger window size**  \n",
    "  The model now looks further back in time to capture more variation.\n",
    "\n",
    "- **Dropout layer**  \n",
    "  Added dropout to improve generalization and reduce overfitting.\n",
    "\n",
    "- **One-shot dense output**  \n",
    "  A Dense layer predicts the entire forecast horizon at once, avoiding rolling forecasts.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f846989f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# represent the number of business days in a year\n",
    "window_size = 252\n",
    "horizon     = 252\n",
    "\n",
    "spark_df = spark_df.select(\"date\", \"close\")\n",
    "spark_df = spark_df.withColumn(\"date\", to_date(col(\"date\"), \"yyyy-MM-dd\"))\n",
    "training_pdf = spark_df.toPandas()\n",
    "\n",
    "train_vals   = training_pdf[\"close\"].values.reshape(-1,1)\n",
    "scaler       = MinMaxScaler()\n",
    "scaled_all   = scaler.fit_transform(train_vals).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e820c7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sliding windows\n",
    "X, Y = [], []\n",
    "for i in range(len(scaled_all) - window_size - horizon + 1):\n",
    "    X.append(scaled_all[i : i + window_size])\n",
    "    Y.append(scaled_all[i + window_size : i + window_size + horizon])\n",
    "X = np.array(X).reshape(-1, window_size, 1)  # (samples, window_size, 1)\n",
    "Y = np.array(Y)                              # (samples, horizon)\n",
    "\n",
    "# model definition \n",
    "model = Sequential([\n",
    "    LSTM(64, input_shape=(window_size, 1)),\n",
    "    Dropout(0.2),\n",
    "    Dense(horizon)      # predict `horizon` steps at once\n",
    "])\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mse\"])\n",
    "history = model.fit(\n",
    "    X, Y,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_split=0.1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Predict on the last window\n",
    "last_window = scaled_all[-window_size:].reshape(1, window_size, 1)\n",
    "pred_scaled = model.predict(last_window)[0]\n",
    "\n",
    "# 5) Inverse scale forecasts\n",
    "pred_prices = scaler.inverse_transform(pred_scaled.reshape(-1, 1)).flatten()\n",
    "\n",
    "# 6) Prepare dates\n",
    "last_date    = training_pdf[\"date\"].iloc[-1]\n",
    "future_dates = pd.bdate_range(start=last_date + BDay(1), periods=horizon)\n",
    "\n",
    "# 7) Plot only the last year of actual data\n",
    "one_year_ago = last_date - pd.DateOffset(years=1)\n",
    "mask_recent  = training_pdf[\"date\"] >= one_year_ago\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(\n",
    "    training_pdf.loc[mask_recent, \"date\"],\n",
    "    training_pdf.loc[mask_recent, \"close\"],\n",
    "    label=\"Historical (Last 1 Year)\"\n",
    ")\n",
    "plt.plot(future_dates, pred_prices, label=\"1-Year Direct Forecast\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Close Price\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
